{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d5830a5",
   "metadata": {},
   "source": [
    "# TAL aplicado al análisis del discurso de los medios de prensa 📰🤓🔥\n",
    "\n",
    "\n",
    "### Cronograma\n",
    "\n",
    "- Hito Unidad 2 (27 de octubre): Implementación y experimentos de varios modelos de clasificación\n",
    "\n",
    "- Hito Proyecto (15 de diciembre): Evaluación y comparación de los modelos de los distintos equipos + integración de los mejores modelos en la arquitectura Sophia2.\n",
    "\n",
    "\n",
    "## índex\n",
    "1. [Importación del dataset](a)\n",
    "2. [Balancear dataset](b)\n",
    "3. [Inicialización del modelo spaCy y tokenización](c)\n",
    "4. [Definición de la arquitectura CNN](d)\n",
    "5. [Funciones para optimizar el modelo](e)\n",
    "6. [Funciones para evaluar el modelo](f)\n",
    "7. [Optimización del modelo](g)\n",
    "8. [Evaluación del modelo](h)   \n",
    "    8.1 [Matriz de confusión](k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0867b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.legacy import data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac66af0",
   "metadata": {},
   "source": [
    "### 1. Importación del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a71733",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"topics.csv\",sep=',',error_bad_lines=False)\n",
    "df.drop(['Unnamed: 0'], axis = 1, inplace=True) # Para suprimir la columna ID\n",
    "df.drop(columns=['text','media_outlet', 'url','topic'], inplace=True)\n",
    "\n",
    "df = df.astype({\"label\": str})\n",
    "df.replace(\"0.0\",\"mundo\", inplace=True)\n",
    "df.replace(\"1.0\",\"economia\", inplace=True)\n",
    "df.replace(\"2.0\",\"politica\", inplace=True)\n",
    "df.replace(\"3.0\",\"ciencia\", inplace=True)\n",
    "df.replace(\"6.0\",\"deporte\", inplace=True)\n",
    "df.replace(\"8.0\",\"crimen\", inplace=True)\n",
    "df.replace(\"9.0\",\"salud\", inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df4e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = df[df['label']!=\"5.0\"]\n",
    "dfc = dfc[dfc['label']!=\"7.0\"]\n",
    "dfc['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ff216e",
   "metadata": {},
   "source": [
    "### 2. Balancear dataset\n",
    "Realizaremos random undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1bcb3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "undersample = RandomUnderSampler(random_state=0)\n",
    "x_res,y_res = undersample.fit_resample(dfc,dfc['label'])\n",
    "fig, ax = plt.subplots(figsize=(4,4),tight_layout=True)\n",
    "ax = y_res.value_counts().plot.pie(autopct='%.2f')\n",
    "_ = ax.set_title(\"Undersampling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cf31af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "valid, test, train = np.split(x_res, [int(.15*len(x_res)), int(.3*len(x_res))])\n",
    "print(\"Largo dataset entrenamiento: \", len(train))\n",
    "print(\"Largo dataset validación: \", len(valid))\n",
    "print(\"Largo dataset test\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa572ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"CNN_train.csv\", encoding=\"UTF-8\",index=False)\n",
    "valid.to_csv(\"CNN_valid.csv\", encoding=\"UTF-8\",index=False)\n",
    "test.to_csv(\"CNN_test.csv\", encoding=\"UTF-8\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817cff8f",
   "metadata": {},
   "source": [
    "### 3. Inicialización del modelo spaCy y tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fa4741",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_es = spacy.load('es_core_news_sm')\n",
    "def tokenize_es(sentence):\n",
    "    return [tok.text for tok in spacy_es.tokenizer(sentence)]\n",
    "\n",
    "\n",
    "\n",
    "TEXT = data.Field(tokenize=tokenize_es, batch_first = True)\n",
    "LABEL = data.LabelField()\n",
    "fields = [('content', TEXT),('label', LABEL)]\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "                                        path = '.',\n",
    "                                        train = 'CNN_train.csv',\n",
    "                                        validation= 'CNN_valid.csv',\n",
    "                                        test = 'CNN_test.csv',\n",
    "                                        format = 'csv',\n",
    "                                        fields = fields,\n",
    "                                        skip_header = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d4bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "#'cuda' if torch.cuda.is_available() else \n",
    "device = torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device,\n",
    "    sort_key=lambda x:len(x.label),\n",
    "    sort_within_batch=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7fe872",
   "metadata": {},
   "source": [
    "### 4. Definición de la arquitectura CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827b0c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://dcc.uchile.cl/~jperez/word-embeddings/glove-sbwc.i25.vec.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aaa424",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 50000\n",
    "\n",
    "## TENER VECTORES EN ESPAÑOL\n",
    "vec = torchtext.vocab.Vectors('glove-sbwc.i25.vec.gz', cache='.')\n",
    "TEXT.build_vocab(train_data, vectors=vec, max_size = MAX_VOCAB_SIZE, unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21c5827",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485666a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "    \n",
    "        super().__init__()\n",
    "    \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv1d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        #text = text.permute(1, 0)    \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)     \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]    \n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37561538",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [2,3,4]\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "\n",
    "OUTPUT_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ce8459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f411ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd32f2c5",
   "metadata": {},
   "source": [
    "### 5. Funciones para optimizar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f9d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #MULTICLASS ---> en lugar de .BCEWithLogitsLoss() (Binary Cross Entropy)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdc8c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #print(\"Tamaño texto de entrada:\"+str(batch.body.shape))\n",
    "        \n",
    "        predictions = model(batch.content)\n",
    "        #print(\"Tamaño predecciones de salida:\"+str(predictions.shape)) \n",
    "        #print(\"Tamaño target:\"+str(batch.category.shape)) \n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        #acc = categorical_accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66918b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8a679",
   "metadata": {},
   "source": [
    "### 6. Funciones para evaluar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d9641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.cuda.FloatTensor([y.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362fffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.content)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e0c0b5",
   "metadata": {},
   "source": [
    "### 7. Optimización del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7351f0dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"inicio optimización\")\n",
    "\n",
    "N_EPOCHS = 1 #5\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        name = './tematic-model-CNN'+'_ep'+str(epoch+1)+'.pt'\n",
    "        torch.save({'epoca': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'Valid_loss': best_valid_loss}, name)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81fd104",
   "metadata": {},
   "source": [
    "### 8. Evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc480df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff38ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "best_model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "best_model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "best_model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e0762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = './tematic-model-CNN'+'_ep'+str(2)+'.pt'\n",
    "best_model.load_state_dict(torch.load(name, map_location=torch.device('cpu'))['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20cc369",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c65dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test = []\n",
    "labels_test=[]\n",
    "for batch in test_iterator:\n",
    "    labels_test.append(batch.category.cpu().detach().numpy())\n",
    "    predictions = best_model(batch.body.cpu()).squeeze(1)\n",
    "    #print(torch.sigmoid(predictions))\n",
    "    rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "    prediction_test.append(rounded_preds.detach().numpy())\n",
    "    #print(prediction_test)\n",
    "    \n",
    "\n",
    "y_true = np.concatenate(labels_test)\n",
    "y_pred = np.concatenate(prediction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca12863c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f0758c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f4dd7b7",
   "metadata": {},
   "source": [
    "#### 8.1 Matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05e3964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def plot_confusion_matrix(cm, labels, cmap=plt.cm.Blues):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5), tight_layout=True)\n",
    "    ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    for i in range(cm.shape[1]):\n",
    "        for j in range(cm.shape[0]):\n",
    "            ax.text(j, i, \"{:,}\".format(cm[i, j]), \n",
    "                    horizontalalignment=\"center\", verticalalignment=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > np.amax(cm)/2 else \"black\")\n",
    "    ax.set_title(\"Matriz de confusión\")\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.ylabel('Etiqueta real')\n",
    "    plt.xlabel('Predicción')\n",
    "\n",
    "cm = confusion_matrix(y_true=test_targets, y_pred=prediction_test)\n",
    "plot_confusion_matrix(cm, labels=[str(i) for i in range(10)])\n",
    "print(classification_report(test_targets, prediction_test, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
