{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c3e123",
   "metadata": {},
   "source": [
    "\n",
    "# 1. Discusión de los resultados\n",
    "\n",
    "## 1.1 Generación del dataset\n",
    "\n",
    "#### Extracción tópicos en URLs\n",
    "\n",
    "Identificamos la morfología de la url de los distintos medios de prensa. En base a esto, extrajimos sus categorías utilizando expresiones regulares.   \n",
    "El resultado de esta operación fue la siguiente:\n",
    "\n",
    "\n",
    "|**N°** | **Categoría** |\n",
    "|---|:---------:|\n",
    "|mundo|8564|\n",
    "|economía|2670|\n",
    "|política y conflictos|3329|\n",
    "|ciencia y tecnología|292|\n",
    "|catástrofes y accidentes|0|\n",
    "|cultura y arte|239|\n",
    "|deporte|7994|\n",
    "|ecología y planeta|162|\n",
    "|crimen, delitos y justicia|537|\n",
    "|salud|362|\n",
    "| **sin clasificar** | 43190|\n",
    "\n",
    "#### Extracción términos clave de un corpus\n",
    "\n",
    "Utilizando Spacy extrajimos los términos clave del título y texto de las noticias sin clasificar.\n",
    "Nos enfocaremos en balancear aquellas categorías con pocas noticias.\n",
    "\n",
    "Trabajamos con dos patrones:\n",
    "1. \"NOUN\" de \"NOUN\"\n",
    "2. \"NOUN\" \"ADJ\"\n",
    "\n",
    "Luego, para cada noticia identificamos su primer y segundo término clave más común. Además, se obtuvo el número de ocurrencias de cada término clave en el dataset y se ordenó de forma descendente. \n",
    "\n",
    "\n",
    "Finalmente, clasificamos de forma manual las noticias considerando su primer y segundo término más común. Por ejemplo, si la noticia tiene como término más común \"incendio forestal\", entonces será clasificada como _catastrofes y accidentes_.\n",
    "\n",
    "El resultado de esta operación nos perminió clasificar un gran número de las noticias restantes.\n",
    "\n",
    "\n",
    "\n",
    "#### Transformers\n",
    "\n",
    "Se trató de crear noticias para las categorías con menor cantidad. Hicimos una extracción random del corpus de varias noticias, y luego utilizando GPT-2  generamos nuevas noticias.   \n",
    "El resultado obtenido no fue el esperado, ya que el texto generado presentaba poca información relevante.   \n",
    "\n",
    "\n",
    "\n",
    "#### Crawling y Scraping\n",
    "\n",
    "Se realizó una extracción de noticias de las siguientes fuentes:\n",
    "\n",
    "- https://www.cultura.gob.cl/noticias/\n",
    "- https://www.minciencia.gob.cl/noticias/ \n",
    "- https://mma.gob.cl/category/noticias/\n",
    "- https://www.emol.com/tag/accidentes/159/todas.aspx\n",
    "\n",
    "\n",
    "## 1.2 Análisis de los modelos\n",
    "\n",
    "Para el entrenamiento de los siguientes modelos se utilizó un dataset compuesto por 2060 noticias aprox. por categoría, mediante oversampling.   \n",
    "(El entrenamiento no contempla las noticias obtenidas mediante scraping).\n",
    "\n",
    "#### Red neuronal convolucional\n",
    "Se generaron datasets de entrenamiento, validación y testeo con un 70%, 15% y 15% de los datos, respectivamente.\n",
    "Los resultados obtenidos en la 5ta época son los siguientes:\n",
    "\n",
    "    Epoch: 05 | Epoch Time: 21m 15s\n",
    "        Train Loss: 0.081 | Train Acc: 97.80%\n",
    "         Val. Loss: 0.223 |  Val. Acc: 93.84%\n",
    "         \n",
    "         \n",
    "Finalmente, el resultado del testeo es el siguiente:\n",
    "\n",
    "        Test Loss: 0.197 | Test Acc: 94.39%\n",
    "        \n",
    "<img src=\"data/graph.jpg\" alt=\"alt text\" width=\"50%\"/>\n",
    "\n",
    "#### Random Forest \n",
    "Se generon datasets de entrenamiento y testeo con 50% de los datos, respectivamente.\n",
    "Los resultados obtenidos son los siguientes:\n",
    "         \n",
    "        Accuracy: 0.8990601135696104\n",
    "        Precision: 0.8990601135696104\n",
    "        Recall: 0.8990601135696104\n",
    "\n",
    "#### Regresores logísticos\n",
    "Se generon datasets de entrenamiento y testeo con 50% de los datos, respectivamente.\n",
    "Los resultados obtenidos son los siguientes:\n",
    "\n",
    "        Accuracy: 0.9278441355002938\n",
    "        Precision: 0.9278441355002938\n",
    "        Recall: 0.9278441355002938\n",
    "        \n",
    "#### Naive Bayes\n",
    "Se generon datasets de entrenamiento y testeo con 50% de los datos, respectivamente.\n",
    "Los resultados obtenidos son los siguientes:\n",
    "\n",
    "        Accuracy: 0.8347201827378039\n",
    "        Precision: 0.8347201827378039\n",
    "        Recall: 0.8347201827378039\n",
    "        \n",
    "#### Conclusión\n",
    "Del estudio podemos decir que el modelo que nos prestó mejores resultados fue CNN con 5 épocas. En cambio, el modelo con un peor resultado fue Naive Bayes.\n",
    "\n",
    "Creemos que este resultado se debe al tamaño del dataset de entrenamiento, en CNN fue más grande en los otros.\n",
    "Por otro lado, las redes neuronales convolucionales tienden a sobreajustarse a los datos debido a su arquitectura y al número de capas que presenta, estas tienden a extraer mayor información relevante para la clasificación.\n",
    "\n",
    "# 2. Nuevos acercamientos \n",
    "\n",
    "### Utilización de APIs para generar el dataset de entrenamiento y traducción\n",
    "\n",
    "Una posibilidad de mejora en la generación del dataset de entrenamiento puede ser mediante la utilización de las APIs de Google News, puesto que permiten extraer fácilmente un gran número de noticias ya clasificadas.\n",
    "\n",
    "- https://gnews.io/\n",
    "- https://newsapi.org/s/google-news-api\n",
    "\n",
    "Sin embargo, estas presentan el inconveniente que entregan noticias en inglés. \n",
    "\n",
    "Esta situación nos hace replantear nuestro acercamiento del proyecto. Creemos factible realizar la clasificación en inglés, con embeddings más robustos.\n",
    "\n",
    "El procedimiento sería el siguiente:\n",
    "\n",
    "Primero, generamos el dataset de entrenamiento realizando consultas a la API sobre las categorías de interés. Podemos extraerlas del ny-times, bbc-news o cualquier otro medio comunicacional. Luego, realizamos el entrenamiento con un modelo CNN, debido a que presentan una mejor precisión.\n",
    "\n",
    "Finalmente, mediante la utilización de transformers o, mediante el uso de la API de google translate (https://cloud.google.com/translate/), se traduce la noticia que queremos clasificar al inglés para que nuestro modelo la clasifique. \n",
    "\n",
    "Cabe mencionar que la mayor limitación de este acercamiento reside en incapacidad del modelo por clasificar la categoría _mundo_, puesto que no estaríamos usando noticias nacionales.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
