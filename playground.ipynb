{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7f650f8",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# TAL aplicado al análisis del discurso de los medios de prensa 📰🤓🔥\n",
    "\n",
    "\n",
    "El proyecto consiste en entrenar y evaluar varios modelos de clasificación supervisada capaz de clasificar una noticia según la taxonomía siguiente: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eea7aa",
   "metadata": {},
   "source": [
    "- mundo = 0 \n",
    "- economía = 1 \n",
    "- política y conflictos = 2 \n",
    "- ciencia y tecnología = 3\n",
    "- catástrofes y accidentes = 4 \n",
    "- cultura y arte = 5 \n",
    "- deporte = 6 \n",
    "- ecología y planeta = 7\n",
    "- crimen, delitos y justicia = 8 \n",
    "- salud = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172a8e0a",
   "metadata": {},
   "source": [
    "- Hito Unidad 1 (29 de septiembre): Datasets de entrenamiento y test + primer modelo baseline\n",
    "\n",
    "- Hito Unidad 2 (27 de octubre): Implementación y experimentos de varios modelos de clasificación\n",
    "\n",
    "- Hito Proyecto (15 de diciembre): Evaluación y comparación de los modelos de los distintos equipos + integración de los mejores modelos en la arquitectura Sophia2.\n",
    "\n",
    "\n",
    "### índex\n",
    "\n",
    "1. [Importación del dataset](a)\n",
    "2. [Extracción de tópicos vía RE en URLs](b)\n",
    "3. [Extracción de términos clave de columnas title+text](b)\n",
    "4. [Clasificación del dataset de entrenamiento](c)\n",
    "5. [Balance de clases dataset de entrenamiento](d)\n",
    "6. [Importación de los dataset restantes](e)\n",
    "7. [Dividir dataset en entrenamiento y test](f)\n",
    "8. [Dividir dataset entrenamiento en entrenamiento y validación](g)\n",
    "\n",
    "## Generando dataset de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22cb9fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "versión de Spacy:  3.1.3\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation\n",
    "import re\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NLP\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec, CoherenceModel\n",
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim_models  # don't skip this\n",
    "import nltk \n",
    "#nltk.download('stopwords') \n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "print(\"versión de Spacy: \", spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fcb00f",
   "metadata": {},
   "source": [
    "### 1. Importación de dataset <a name=\"a\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786b14a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largo de dataset:  13965\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"data/chile_2020-09.csv\")\n",
    "dataset['content'] = dataset['title'] + \" \" + dataset['text'] \n",
    "\n",
    "print(\"Largo de dataset: \", len(dataset))\n",
    "for index, row in dataset.iterrows():\n",
    "    if pd.isna(row['text']):\n",
    "        dataset.drop(index, inplace=True)\n",
    "        \n",
    "dataset.drop(columns=['country','year','date','id', 'id_journalist','title','text'], inplace=True)\n",
    "\n",
    "dataset['value'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15176f7",
   "metadata": {},
   "source": [
    "### 2.  Extracción de tópicos vía RE en URL <a name=\"b\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5514030d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     10949\n",
       "0      823\n",
       "2      763\n",
       "6      597\n",
       "8      128\n",
       "1       74\n",
       "7       38\n",
       "5       25\n",
       "9       18\n",
       "Name: value, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in dataset.iterrows():\n",
    "    url=row['url']\n",
    "    obj = re.findall('(\\w+)://([\\w\\-\\.]+)/([\\w\\-]+).([\\w\\-]+)', url) \n",
    "    topic=obj[0][2]\n",
    "    dataset.loc[index,'topic'] = topic\n",
    "    \n",
    "    if topic == 'mundo': \n",
    "        row['value'] = 0\n",
    "    if topic == 'economia': \n",
    "        row['value'] = 1\n",
    "    if topic == 'negocios': \n",
    "        row['value'] = 1      \n",
    "    if topic == 'politica': \n",
    "        row['value'] = 2\n",
    "    if topic == 'ciencia': \n",
    "        row['value'] = 3\n",
    "    if topic == 'tecnologia': \n",
    "        row['value'] = 3    \n",
    "    if topic == 'catastrofes': \n",
    "        row['value'] = 4      \n",
    "    if topic == 'cultura': \n",
    "        row['value'] = 5\n",
    "    if topic == 'artes': \n",
    "        row['value'] = 5\n",
    "    if topic == 'deportes': \n",
    "        row['value'] = 6\n",
    "    if topic == 'el-deportivo': \n",
    "        row['value'] = 6\n",
    "    if topic == 'medio-ambiente': \n",
    "        row['value'] = 7\n",
    "    if topic == 'justicia': \n",
    "        row['value'] = 8\n",
    "    if topic =='reportaje-investigacion': \n",
    "        row['value'] = 8    \n",
    "    if topic == 'salud': \n",
    "        row['value'] = 9 \n",
    "\n",
    "dataset['value'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9af4d0",
   "metadata": {},
   "source": [
    "Tenemos muchas noticias no clasificadas, por lo que es necesario utilizar otro mecanísmo para clasificar las noticias restantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ab702c",
   "metadata": {},
   "source": [
    "### 3.  Extracción de términos clave de columna title+text <a name=\"b\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb9634",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Pattern 1: NOUN de NOUN\n",
    "pattern_1 = [{\"POS\": \"NOUN\"},{\"LOWER\": \"de\"}, {\"POS\": \"NOUN\"}]\n",
    "matcher.add(\"NOUN-de-NOUN\", [pattern_1])\n",
    "\n",
    "# Pattern 2: NOUN ADJ\n",
    "pattern_2 = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADJ\"}]\n",
    "matcher.add(\"NOUN-ADJ\", [pattern_2])\n",
    "\n",
    "# Pattern 3: \n",
    "pattern_3 = [{\"POS\": \"NOUN\"}]\n",
    "matcher.add(\"NOUN\", [pattern_3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a110d7e",
   "metadata": {},
   "source": [
    "    Bloque de alto coste computacional ⬇️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e12b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['terms'] = \"\"\n",
    "for index, row in dataset.iterrows():\n",
    "    doc = nlp(row['content'].lower())\n",
    "\n",
    "    matches = matcher(doc)\n",
    "    categories = \"\"\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        categories = categories + span.text + \"; \"\n",
    "    \n",
    "    row['terms'] = categories\n",
    "    \n",
    "display(dataset.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c423a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "dataset[\"most_common\"] = \"\"\n",
    "for index, row in dataset.iterrows():\n",
    "    split_it = row['terms'].split(\"; \")\n",
    "    counter = Counter(split_it)\n",
    "    row['most_common'] = counter.most_common()[0][0]\n",
    "\n",
    "display(dataset.head(5),dataset['most_common'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5897d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28efefee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc2955c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d34b8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4121406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f402b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f324d9d",
   "metadata": {},
   "source": [
    "### 4. Clasificación del dataset de entrenamiento <a name=\"c\"></a>\n",
    "Aplicaremos LDA a cada noticia y extraeremos sus tópicos, luego evaluaremos esos tópicos con las categorias deseadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07db1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['category'] = \"\"\n",
    "#for index, row in dataset.iterrows():\n",
    "#    for index in topics:\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6ec148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "387e5f1d",
   "metadata": {},
   "source": [
    "### 5.  Balance de clases dataset de entrenamiento <a name=\"d\"></a>\n",
    "Visualizamos la densidad de los tópicos con tal de combatir el sobreajuste si corresponde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5187e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(9,5), tight_layout=True)\n",
    "ax.hist(dataset['category'],edgecolor = 'black',bins=dataset['category'].unique())\n",
    "plt.xticks(rotation='vertical', fontsize=7)\n",
    "plt.ylabel(\"densidad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb902858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49017fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a2834a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f8c862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447d1964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3340491c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3894dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc9edd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67841f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
